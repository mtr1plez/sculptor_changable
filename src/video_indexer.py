# -*- coding: utf-8 -*-
# src/video_indexer.py
import os
import cv2
import json
import torch
import numpy as np
from PIL import Image
from tqdm import tqdm
from scenedetect import VideoManager, SceneManager
from scenedetect.detectors import ContentDetector
from sentence_transformers import SentenceTransformer

from utils import load_config

def detect_scenes(video_path, threshold=27.0, min_duration=1.0, start_offset=0.2):
    """
    –®–∞–≥ 1: –ù–∞—Ä–µ–∑–∫–∞ –≤–∏–¥–µ–æ –Ω–∞ —Å—Ü–µ–Ω—ã
    
    Args:
        start_offset: –°–º–µ—â–µ–Ω–∏–µ –Ω–∞—á–∞–ª–∞ —Å—Ü–µ–Ω—ã –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.2)
                     –ö–æ–º–ø–µ–Ω—Å–∏—Ä—É–µ—Ç —Ä–∞–Ω–Ω–µ–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏–µ –¥–µ—Ç–µ–∫—Ç–æ—Ä–∞
    """
    print(f"‚úÇÔ∏è –ò—â–µ–º —Å—Ü–µ–Ω—ã –≤ {os.path.basename(video_path)}...")
    
    video_manager = VideoManager([video_path])
    scene_manager = SceneManager()
    scene_manager.add_detector(ContentDetector(threshold=threshold, min_scene_len=min_duration))
    
    video_manager.start()
    scene_manager.detect_scenes(frame_source=video_manager, show_progress=True)
    scene_list = scene_manager.get_scene_list()
    
    scenes = []
    scene_id = 0
    
    for start, end in scene_list:
        # –ö–†–ò–¢–ò–ß–ù–û: –ü—Ä–∏–º–µ–Ω—è–µ–º offset –∫ start_time
        start_time = start.get_seconds() + start_offset
        end_time = end.get_seconds()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ—Å–ª–µ offset —Å—Ü–µ–Ω–∞ –Ω–µ —Å—Ç–∞–ª–∞ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–π
        duration = end_time - start_time
        
        if duration < min_duration:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ü–µ–Ω—É {scene_id}: —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è –ø–æ—Å–ª–µ offset ({duration:.2f}s)")
            continue
        
        # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ start –Ω–µ –≤—ã—à–µ–ª –∑–∞ end
        if start_time >= end_time:
            print(f"‚ö†Ô∏è –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å—Ü–µ–Ω—É {scene_id}: start >= end –ø–æ—Å–ª–µ offset")
            continue
            
        scenes.append({
            "id": scene_id,
            "start_time": start_time,
            "end_time": end_time,
            "duration": duration,
            "frame_path": ""
        })
        scene_id += 1
    
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(scenes)} —Å—Ü–µ–Ω (—Å offset +{start_offset}s).")
    return scenes

def extract_frames(video_path, scenes, output_dir, image_size=224):
    """–®–∞–≥ 2: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–∞–¥—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ü–µ–Ω—ã"""
    print("üì∏ –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–∞–¥—Ä—ã...")
    cap = cv2.VideoCapture(video_path)
    
    if not cap.isOpened():
        raise IOError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ—Ç–∫—Ä—ã—Ç—å –≤–∏–¥–µ–æ: {video_path}")

    valid_scenes = []

    for scene in tqdm(scenes):
        # –ë–µ—Ä–µ–º –∫–∞–¥—Ä –∏–∑ —Å–µ—Ä–µ–¥–∏–Ω—ã —Å—Ü–µ–Ω—ã
        mid_time = scene["start_time"] + (scene["duration"] / 2)
        
        # –ù–û–í–û–ï: –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º—è –∫–ª—é—á–µ–≤–æ–≥–æ –∫–∞–¥—Ä–∞ –¥–ª—è —Ç–æ—á–Ω–æ–π —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ –≤ XML
        scene["key_frame_time"] = mid_time
        
        # –ü–µ—Ä–µ–º–∞—Ç—ã–≤–∞–µ–º –≤–∏–¥–µ–æ –Ω–∞ –Ω—É–∂–Ω—ã–π –º–æ–º–µ–Ω—Ç (–≤ –º–∏–ª–ª–∏—Å–µ–∫—É–Ω–¥–∞—Ö)
        cap.set(cv2.CAP_PROP_POS_MSEC, mid_time * 1000)
        ret, frame = cap.read()
        
        if ret:
            # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º BGR (OpenCV) -> RGB (PIL)
            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            img = Image.fromarray(frame_rgb)
            
            # –†–µ—Å–∞–π–∑ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞ –∏ —Å–∫–æ—Ä–æ—Å—Ç–∏
            img = img.resize((image_size, image_size))
            
            filename = f"scene_{scene['id']}.jpg"
            filepath = os.path.join(output_dir, filename)
            relative_path = os.path.relpath(filepath)
            img.save(filepath, quality=80)
            
            scene["frame_path"] = relative_path
            valid_scenes.append(scene)
        else:
            print(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ—á–∏—Ç–∞—Ç—å –∫–∞–¥—Ä –¥–ª—è —Å—Ü–µ–Ω—ã {scene['id']}")

    cap.release()
    return valid_scenes

def embed_scenes(scenes, model_name, device):
    """–®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–æ–≤ —á–µ—Ä–µ–∑ CLIP"""
    print(f"üß† –ó–∞–≥—Ä—É–∂–∞–µ–º CLIP ({model_name}) –Ω–∞ {device}...")
    model = SentenceTransformer(model_name, device=device)
    
    image_paths = [s["frame_path"] for s in scenes]
    
    print("‚ö° –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ (–≤–µ–∫—Ç–æ—Ä—ã)...")
    
    # –ò–°–ü–†–ê–í–õ–ï–ù–û: –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –±–∞—Ç—á–∞–º–∏, —á—Ç–æ–±—ã –Ω–µ –¥–µ—Ä–∂–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã –æ—Ç–∫—Ä—ã—Ç—ã–º–∏
    batch_size = 32
    all_embeddings = []
    
    for i in tqdm(range(0, len(image_paths), batch_size), desc="Encoding batches"):
        batch_paths = image_paths[i:i + batch_size]
        
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –±–∞—Ç—á–∞
        images = []
        for path in batch_paths:
            img = Image.open(path)
            images.append(img)
        
        # –ö–æ–¥–∏—Ä—É–µ–º –±–∞—Ç—á
        batch_embeddings = model.encode(
            images,
            batch_size=batch_size,
            convert_to_tensor=False,
            show_progress_bar=False
        )
        
        all_embeddings.append(batch_embeddings)
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        for img in images:
            img.close()
    
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ –±–∞—Ç—á–∏
    embeddings = np.vstack(all_embeddings)
    
    return embeddings

def run_indexer():
    cfg = load_config()
    
    video_path = cfg["paths"]["input_video"]
    
    if not os.path.exists(video_path):
        base, _ = os.path.splitext(video_path)
        mp4_path = base + ".mp4"
        mkv_path = base + ".mkv"

        if os.path.exists(mp4_path):
            video_path = mp4_path
        elif os.path.exists(mkv_path):
            video_path = mkv_path
        else:
            raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω–æ –≤–∏–¥–µ–æ –Ω–∏ {mp4_path}, –Ω–∏ {mkv_path}")
        
    cache_dir = cfg["paths"]["cache_dir"]
    frames_dir = cfg["paths"]["frames_dir"]
    index_path = os.path.join(cache_dir, "scene_index.json")
    emb_path = os.path.join(cache_dir, "embeddings.npy")

    # 0. –ü—Ä–æ–≤–µ—Ä–∫–∞: –ï—Å–ª–∏ –∏–Ω–¥–µ–∫—Å —É–∂–µ –µ—Å—Ç—å, –Ω–µ –¥–µ–ª–∞–µ–º —Ä–∞–±–æ—Ç—É –¥–≤–∞–∂–¥—ã
    if os.path.exists(index_path) and os.path.exists(emb_path):
        print("üìÇ –ò–Ω–¥–µ–∫—Å —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—é.")
        # –¢—É—Ç –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É "force update", –µ—Å–ª–∏ –Ω–∞–¥–æ
        return

    # 1. –î–µ—Ç–µ–∫—Ü–∏—è
    scenes = detect_scenes(
        video_path, 
        threshold=cfg["params"]["scene_threshold"],
        min_duration=cfg["params"]["min_scene_duration"]
    )
    
    # 2. –≠–∫—Å—Ç—Ä–∞–∫—Ü–∏—è –∫–∞–¥—Ä–æ–≤
    scenes = extract_frames(
        video_path, 
        scenes, 
        frames_dir, 
        image_size=cfg["params"]["image_size"]
    )
    
    # 3. –≠–º–±–µ–¥–¥–∏–Ω–≥
    embeddings = embed_scenes(
        scenes, 
        cfg["models"]["clip_model"], 
        cfg["models"]["device"]
    )
    
    # 4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("üíæ –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ...")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON (–º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ)
    with open(index_path, "w", encoding="utf-8") as f:
        json.dump(scenes, f, indent=4)
        
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º NPY (–≤–µ–∫—Ç–æ—Ä—ã)
    np.save(emb_path, embeddings)
    
    print("üéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    # –î–ª—è —Ç–µ—Å—Ç–∞ –∑–∞–ø—É—Å–∫–∞–µ–º —Ñ—É–Ω–∫—Ü–∏—é –Ω–∞–ø—Ä—è–º—É—é
    run_indexer()